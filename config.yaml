# TARS-WIZARD Configuration
# All hardware settings, servo values, and preferences in one place.
# API keys are loaded from .env (not this file).

# ─── Servo Hardware ─────────────────────────────────────────
# PCA9685 servo controller settings.
# DO NOT change these values unless you've recalibrated your servos.
servo:
  i2c_address: 0x40
  frequency: 50
  channels:
    torso: 0
    left_arm: 3
    right_arm: 4
  positions:
    up_height: 130
    neutral_height: 0
    down_height: -130
    forward: 130
    neutral: 0
    backward: -130
    left_arm_neutral: -28
    right_arm_neutral: 28
  # Pulse width range (microseconds) — standard for most hobby servos
  pulse:
    min: 1000
    max: 2000

# ─── Movement Timing ────────────────────────────────────────
# Sleep durations (seconds) between servo commands during movement.
# These are tuned for LD-3015MG servos. Adjust if using different servos.
movement:
  step_delay: 0.2
  arm_delay: 0.3
  settle_delay: 0.4          # pause after returning to neutral (prevents overshoot swing)

# ─── Voice Settings ─────────────────────────────────────────
voice:
  # Speech-to-text
  listen_timeout: 3          # seconds to wait for speech
  # Text-to-speech (edge-tts — free, no API key needed)
  # Rate uses percent format: "+0%" is normal, "-15%" is slower
  # Pitch uses Hz format: "+0Hz" is normal, "-10Hz" is lower
  speech_rate: "-10%"
  speech_pitch: "-20Hz"
  # Post-processing (pydub) — these effects make the voice sound robotic like TARS
  playback_speed: 1.2
  low_pass_filter: 3000       # Hz
  high_pass_filter: 300       # Hz — band-pass with low_pass gives metallic TARS feel
  volume_reduction: 3         # dB
  volume_boost: 8             # dB

# ─── Wake Word ─────────────────────────────────────────────
wake_word:
  threshold: 0.5              # Detection confidence (0.0-1.0)

# ─── Local LLM (Ollama) ───────────────────────────────────
# Offline fallback when internet is unavailable.
# Install Ollama: https://ollama.ai  then run: ollama pull phi3
local_llm:
  base_url: "http://localhost:11434"
  model: phi3                 # Small model that runs on Pi 5 (8GB RAM)

# ─── Language Support ───────────────────────────────────────
# voice_id values are edge-tts voice names (run `edge-tts --list-voices` to see all)
# en-US-GuyNeural is a deep male voice — combined with the effects above, sounds like TARS
languages:
  english:
    voice_id: en-US-GuyNeural
    movement_messages:
      forward: "Moving forward"
      left: "Turning left"
      right: "Turning right"
      neutral: "Returning to neutral position"
  spanish:
    voice_id: es-ES-AlvaroNeural
    movement_messages:
      forward: "Moviendo hacia adelante"
      left: "Girando a la izquierda"
      right: "Girando a la derecha"
      neutral: "Volviendo a posicion neutral"
  french:
    voice_id: fr-FR-HenriNeural
    movement_messages:
      forward: "Avancer"
      left: "Tourner a gauche"
      right: "Tourner a droite"
      neutral: "Retour a la position neutre"
  german:
    voice_id: de-DE-ConradNeural
    movement_messages:
      forward: "Vorwarts bewegen"
      left: "Nach links drehen"
      right: "Nach rechts drehen"
      neutral: "Zuruck zur neutralen Position"
  italian:
    voice_id: it-IT-DiegoNeural
    movement_messages:
      forward: "Movimento in avanti"
      left: "Girando a sinistra"
      right: "Girando a destra"
      neutral: "Ritorno alla posizione neutra"
  portuguese:
    voice_id: pt-BR-AntonioNeural
    movement_messages:
      forward: "Movendo para frente"
      left: "Virando a esquerda"
      right: "Virando a direita"
      neutral: "Retornando a posicao neutra"
  japanese:
    voice_id: ja-JP-KeitaNeural
    movement_messages:
      forward: "前進します"
      left: "左に曲がります"
      right: "右に曲がります"
      neutral: "中立位置に戻ります"

# ─── Personality ────────────────────────────────────────────
personality:
  humor: 50          # 0-100
  honesty: 50        # 0-100
  default_language: english

# ─── AI Model ──────────────────────────────────────────────
# Cerebras is the primary backend (fastest inference).
# OpenAI is the fallback. Ollama is the offline fallback.
ai:
  cerebras_model: llama3.1-8b   # Fast, lightweight — ideal for TARS one-liners
  model: gpt-4o-mini            # OpenAI fallback model
  max_tokens: 60                # Short but not truncated — TARS speaks in 1-2 sentences
  temperature: 0.9

# ─── Weather ────────────────────────────────────────────────
weather:
  api_url: "https://api.openweathermap.org/data/2.5/weather"
  units: metric

# ─── Camera ────────────────────────────────────────────────
# Pi Camera Module v2 via picamera2.
# YOLO runs locally for object/person detection.
camera:
  resolution: [640, 480]       # Capture resolution (width, height)
  yolo_model: yolov8n.pt       # Nano model — fast on Pi 5
  yolo_confidence: 0.5         # Min detection confidence (0.0-1.0)

# ─── Controller ─────────────────────────────────────────────
# Auto-detected on startup. Override here if needed.
controller:
  device_path: auto        # "auto" scans /dev/input/, or set explicit path
  button_map:
    move_forward: BTN_A
    turn_right: BTN_B
    turn_left: BTN_X
    neutral: BTN_Y
    stop: BTN_START
